{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f153cc11-591b-4c72-b91a-6b37305caa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "import math\n",
    "def load_data(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            sentence = row[0]\n",
    "            tokens = sentence.strip().split()\n",
    "            data.append({\"text\": tokens})\n",
    "    return data\n",
    "\n",
    "train_data = load_data(\"train.tsv\")\n",
    "dev_data = load_data(\"dev.tsv\")\n",
    "test_data = load_data(\"test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d31ef7d0-81df-40dd-8e1b-6eaa7749f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = set()\n",
    "for sent in train_data:\n",
    "    train_words.update(sent[\"text\"])\n",
    "\n",
    "special_tokens = [\"<bos>\", \"<eos>\", \"<unk>\"]\n",
    "for st in special_tokens:\n",
    "    train_words.add(st)\n",
    "class WordDict:\n",
    "    def __init__(self, words):\n",
    "        assert type(words) == set\n",
    "        words = list(words)\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = []\n",
    "        \n",
    "        for i, w in enumerate(words):\n",
    "            self.word_to_idx[w] = i\n",
    "            self.idx_to_word.append(w)\n",
    "        \n",
    "        self.unk_idx = self.word_to_idx[\"<unk>\"]\n",
    "        self.bos_idx = self.word_to_idx[\"<bos>\"]\n",
    "        self.eos_idx = self.word_to_idx[\"<eos>\"]\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        assert type(word) == str\n",
    "        return self.word_to_idx.get(word, self.unk_idx)\n",
    "\n",
    "    def id_to_word(self, idx):\n",
    "        assert type(idx) == int\n",
    "        return self.idx_to_word[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_word)\n",
    "\n",
    "word_dict = WordDict(train_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2840a01-74f9-4372-95d1-edb9e8130d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_unk_replace(ids, unk_idx, p_unk=0.01):\n",
    "    new_ids = []\n",
    "    for w in ids:\n",
    "        if w not in [word_dict.bos_idx, word_dict.eos_idx] and random.random() < p_unk:\n",
    "            new_ids.append(unk_idx)\n",
    "        else:\n",
    "            new_ids.append(w)\n",
    "    return new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f01b3af5-0672-411c-8554-4d89d70fef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Perplexity:\n",
    "    def __init__(self):\n",
    "        self.log_sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.log_sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def add_sentence(self, log_probs):\n",
    "        self.log_sum += log_probs.sum().item()\n",
    "        self.count += log_probs.size(0)\n",
    "\n",
    "    def compute_perplexity(self):\n",
    "        return math.exp(-self.log_sum / self.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "365f306e-f912-4f10-b4fa-6c6398c13684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ngram_data(data, word_dict, n=2, p_unk=0.01):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in data:\n",
    "        tokens = [word_dict.bos_idx]*(n-1) + [word_dict.word_to_id(w) for w in sentence[\"text\"]] + [word_dict.eos_idx]\n",
    "        tokens = random_unk_replace(tokens, word_dict.unk_idx, p_unk=p_unk)\n",
    "        for i in range(len(tokens)-(n-1)):\n",
    "            x = tokens[i:i+(n-1)]\n",
    "            y = tokens[i+(n-1)]\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "    return torch.LongTensor(X), torch.LongTensor(Y)\n",
    "N=3\n",
    "X_train, Y_train = prepare_ngram_data(train_data, word_dict, n=N, p_unk=0.01)\n",
    "X_dev, Y_dev = prepare_ngram_data(dev_data, word_dict, n=N, p_unk=0.0)   # No unk replacement for dev/test\n",
    "X_test, Y_test = prepare_ngram_data(test_data, word_dict, n=N, p_unk=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6575f90-9acc-412c-ac58-35bd06f9549c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 7.0700, Dev Perplexity = 624.4998\n",
      "Epoch 1: Loss 5.7125, Dev Perplexity = 575.6412\n",
      "Epoch 2: Loss 6.5033, Dev Perplexity = 629.9645\n",
      "Epoch 3: Loss 5.3174, Dev Perplexity = 708.0984\n",
      "Epoch 4: Loss 6.3150, Dev Perplexity = 823.2724\n",
      "Epoch 5: Loss 5.0901, Dev Perplexity = 916.6693\n",
      "Epoch 6: Loss 6.5045, Dev Perplexity = 1017.5499\n",
      "Epoch 7: Loss 5.4527, Dev Perplexity = 1140.1235\n",
      "Epoch 8: Loss 4.9986, Dev Perplexity = 1275.3129\n",
      "Epoch 9: Loss 6.4656, Dev Perplexity = 1469.8640\n",
      "Epoch 10: Loss 5.9034, Dev Perplexity = 1495.1667\n",
      "Epoch 11: Loss 5.7650, Dev Perplexity = 1656.7952\n",
      "Epoch 12: Loss 5.4157, Dev Perplexity = 1815.3043\n",
      "Epoch 13: Loss 6.6135, Dev Perplexity = 1890.6309\n",
      "Epoch 14: Loss 6.6688, Dev Perplexity = 2019.1846\n",
      "Epoch 15: Loss 5.9038, Dev Perplexity = 2142.1057\n",
      "Epoch 16: Loss 5.5457, Dev Perplexity = 2305.3557\n",
      "Epoch 17: Loss 4.8988, Dev Perplexity = 2557.6049\n",
      "Epoch 18: Loss 5.6336, Dev Perplexity = 2739.9702\n",
      "Epoch 19: Loss 5.5487, Dev Perplexity = 2951.1828\n",
      "Total training time for 20 epochs: 24612.60 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, n=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.fc1 = nn.Linear((n-1)*embed_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        emb = self.embed(x) \n",
    "        emb = emb.view(emb.size(0), -1)\n",
    "        h = F.relu(self.fc1(emb))\n",
    "        logits = self.fc2(h)\n",
    "        return logits\n",
    "\n",
    "# Train N-Gram Model\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "model = NGramModel(len(word_dict), embed_size=100, hidden_size=100, n=N)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "start_training_time = time.time()\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, Y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_train_loss=loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        perplex = Perplexity()\n",
    "        dev_dataset = TensorDataset(X_dev, Y_dev)\n",
    "        dev_loader = DataLoader(dev_dataset, batch_size=64, shuffle=False)\n",
    "        for X_batch, Y_batch in dev_loader:\n",
    "            logits = model(X_batch)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            chosen_log_probs = log_probs[range(len(Y_batch)), Y_batch]\n",
    "            perplex.add_sentence(chosen_log_probs)\n",
    "        dev_perplexity= perplex.compute_perplexity()\n",
    "    print(f\"Epoch {epoch}: Loss {avg_train_loss:.4f}, Dev Perplexity = {dev_perplexity:.4f}\")\n",
    "end_training_time = time.time()\n",
    "total_time = end_training_time - start_training_time\n",
    "print(f\"Total training time for {20} epochs: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d99800dc-ec67-4b64-bff3-c467621bc6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Gram Test Perplexity: 2875.43669650205\n"
     ]
    }
   ],
   "source": [
    "# Evaluate perplexity on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    perplex = Perplexity()\n",
    "    test_dataset = TensorDataset(X_test, Y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    for X_batch, Y_batch in test_loader:\n",
    "        logits = model(X_batch)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        chosen_log_probs = log_probs[range(len(Y_batch)), Y_batch]\n",
    "        perplex.add_sentence(chosen_log_probs)\n",
    "    print(\"N-Gram Test Perplexity:\", perplex.compute_perplexity())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68f768f0-bb9e-41f7-bf0d-ea90d8aad212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 6.5315, Dev Perplexity = 586.6392\n",
      "Epoch 1: Loss 6.1477, Dev Perplexity = 533.1035\n",
      "Epoch 2: Loss 6.4616, Dev Perplexity = 577.0679\n",
      "Epoch 3: Loss 5.6897, Dev Perplexity = 647.0101\n",
      "Epoch 4: Loss 6.4187, Dev Perplexity = 708.8537\n",
      "Epoch 5: Loss 4.6465, Dev Perplexity = 754.3142\n",
      "Epoch 6: Loss 5.9211, Dev Perplexity = 835.5743\n",
      "Epoch 7: Loss 6.1655, Dev Perplexity = 911.8732\n",
      "Epoch 8: Loss 6.0267, Dev Perplexity = 948.0066\n",
      "Epoch 9: Loss 4.9254, Dev Perplexity = 1037.9835\n",
      "Epoch 10: Loss 5.6900, Dev Perplexity = 1061.2460\n",
      "Epoch 11: Loss 5.4448, Dev Perplexity = 1163.6498\n",
      "Epoch 12: Loss 5.4073, Dev Perplexity = 1210.4669\n",
      "Epoch 13: Loss 5.6467, Dev Perplexity = 1242.6342\n",
      "Epoch 14: Loss 5.4322, Dev Perplexity = 1347.8352\n",
      "Epoch 15: Loss 4.9357, Dev Perplexity = 1340.5067\n",
      "Epoch 16: Loss 5.9241, Dev Perplexity = 1408.9623\n",
      "Epoch 17: Loss 5.8435, Dev Perplexity = 1478.0010\n",
      "Epoch 18: Loss 5.1663, Dev Perplexity = 1557.3239\n",
      "Epoch 19: Loss 5.8911, Dev Perplexity = 1601.6762\n",
      "Total training time for 20 epochs: 24541.49 seconds\n"
     ]
    }
   ],
   "source": [
    "N=2\n",
    "X_train, Y_train = prepare_ngram_data(train_data, word_dict, n=N, p_unk=0.01)\n",
    "X_dev, Y_dev = prepare_ngram_data(dev_data, word_dict, n=N, p_unk=0.0)   # No unk replacement for dev/test\n",
    "X_test, Y_test = prepare_ngram_data(test_data, word_dict, n=N, p_unk=0.0)\n",
    "class NGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, n=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.fc1 = nn.Linear((n-1)*embed_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        emb = self.embed(x) \n",
    "        emb = emb.view(emb.size(0), -1)\n",
    "        h = F.relu(self.fc1(emb))\n",
    "        logits = self.fc2(h)\n",
    "        return logits\n",
    "\n",
    "# Train N-Gram Model\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "model = NGramModel(len(word_dict), embed_size=100, hidden_size=100, n=N)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "start_training_time = time.time()\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, Y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_train_loss=loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        perplex = Perplexity()\n",
    "        dev_dataset = TensorDataset(X_dev, Y_dev)\n",
    "        dev_loader = DataLoader(dev_dataset, batch_size=64, shuffle=False)\n",
    "        for X_batch, Y_batch in dev_loader:\n",
    "            logits = model(X_batch)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            chosen_log_probs = log_probs[range(len(Y_batch)), Y_batch]\n",
    "            perplex.add_sentence(chosen_log_probs)\n",
    "        dev_perplexity= perplex.compute_perplexity()\n",
    "    print(f\"Epoch {epoch}: Loss {avg_train_loss:.4f}, Dev Perplexity = {dev_perplexity:.4f}\")\n",
    "end_training_time = time.time()\n",
    "total_time = end_training_time - start_training_time\n",
    "print(f\"Total training time for {20} epochs: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82550eba-3c68-4385-9ef7-0b045c3bb026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Gram Test Perplexity: 1565.1502315444286\n"
     ]
    }
   ],
   "source": [
    "# Evaluate perplexity on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    perplex = Perplexity()\n",
    "    test_dataset = TensorDataset(X_test, Y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    for X_batch, Y_batch in test_loader:\n",
    "        logits = model(X_batch)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        chosen_log_probs = log_probs[range(len(Y_batch)), Y_batch]\n",
    "        perplex.add_sentence(chosen_log_probs)\n",
    "    print(\"N-Gram Test Perplexity:\", perplex.compute_perplexity())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ba30a8b-24b1-44d6-be26-e7b1d7509c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_id_sequences(data, word_dict, p_unk=0.0):\n",
    "    sequences = []\n",
    "    for sentence in data:\n",
    "        seq = [word_dict.bos_idx] + [word_dict.word_to_id(w) for w in sentence[\"text\"]] + [word_dict.eos_idx]\n",
    "        if p_unk > 0:\n",
    "            seq = random_unk_replace(seq, word_dict.unk_idx, p_unk=p_unk)\n",
    "        sequences.append(torch.LongTensor(seq))\n",
    "    return sequences\n",
    "\n",
    "train_seq = to_id_sequences(train_data, word_dict, p_unk=0.01)\n",
    "dev_seq = to_id_sequences(dev_data, word_dict, p_unk=0.0)\n",
    "test_seq = to_id_sequences(test_data, word_dict, p_unk=0.0)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = sorted(batch, key=lambda x: len(x), reverse=True)\n",
    "    lengths = torch.LongTensor([len(x) for x in batch])\n",
    "    padded = pad_sequence(batch, batch_first=True, padding_value=word_dict.bos_idx)\n",
    "    X = padded[:, :-1]\n",
    "    Y = padded[:, 1:]\n",
    "    #print(lengths)\n",
    "    return X, Y, lengths\n",
    "    \n",
    "train_loader = DataLoader(train_seq, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_seq, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_seq, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c8dd437-f85a-4bc3-8b1c-41e4087c6930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5426\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_seq))\n",
    "print(len(dev_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0147745-f084-4363-922c-0dce542cd6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        if (not self.training) or self.p == 0.0:\n",
    "            return x\n",
    "        mask = x.new_empty(x.size(0), 1, x.size(2)).bernoulli_(1 - self.p) / (1 - self.p)\n",
    "        return x * mask\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.input_dropout = VariationalDropout(dropout)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.output_dropout = VariationalDropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.embed(x)\n",
    "        emb = self.input_dropout(emb)\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        output = self.output_dropout(output)\n",
    "        logits = self.fc(output)\n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07b15d50-e660-4118-8be9-5c77933bc7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 7.1042, Dev Perplexity = 742.1433\n",
      "Epoch 1: Loss 6.8670, Dev Perplexity = 573.4292\n",
      "Epoch 2: Loss 7.2692, Dev Perplexity = 491.6572\n",
      "Epoch 3: Loss 6.4524, Dev Perplexity = 440.2803\n",
      "Epoch 4: Loss 6.6398, Dev Perplexity = 409.1527\n",
      "Epoch 5: Loss 6.2628, Dev Perplexity = 387.0798\n",
      "Epoch 6: Loss 6.3093, Dev Perplexity = 369.6944\n",
      "Epoch 7: Loss 6.5122, Dev Perplexity = 357.7820\n",
      "Epoch 8: Loss 6.3525, Dev Perplexity = 347.8421\n",
      "Epoch 9: Loss 6.4224, Dev Perplexity = 338.4932\n",
      "Epoch 10: Loss 6.2977, Dev Perplexity = 333.0394\n",
      "Epoch 11: Loss 6.2724, Dev Perplexity = 325.7916\n",
      "Epoch 12: Loss 6.6485, Dev Perplexity = 321.1315\n",
      "Epoch 13: Loss 5.9172, Dev Perplexity = 318.5404\n",
      "Epoch 14: Loss 6.3668, Dev Perplexity = 314.1384\n",
      "Epoch 15: Loss 6.0237, Dev Perplexity = 310.5050\n",
      "Epoch 16: Loss 6.0560, Dev Perplexity = 310.1268\n",
      "Epoch 17: Loss 6.7673, Dev Perplexity = 307.4113\n",
      "Epoch 18: Loss 5.9179, Dev Perplexity = 305.1077\n",
      "Epoch 19: Loss 6.0298, Dev Perplexity = 303.9958\n",
      "Total training time for 20 epochs: 17480.45 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "model_lstm = LSTMModel(len(word_dict), embed_size=100, hidden_size=100, dropout=0.5)\n",
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_dict.bos_idx)\n",
    "start_training_time = time.time()\n",
    "for epoch in range(20):\n",
    "    model_lstm.train()\n",
    "    for X_batch, Y_batch, lengths in train_loader:\n",
    "        logits, _ = model_lstm(X_batch)\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), Y_batch.reshape(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_train_loss=loss.item()\n",
    "    \n",
    "    model_lstm.eval()\n",
    "    with torch.no_grad():\n",
    "        perplex = Perplexity()\n",
    "        for X_batch, Y_batch, lengths in dev_loader:\n",
    "            logits, _ = model_lstm(X_batch)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            chosen_log_probs = log_probs.reshape(-1, log_probs.size(-1))[range(Y_batch.numel()), Y_batch.reshape(-1)]\n",
    "            mask = (Y_batch != word_dict.bos_idx).view(-1)\n",
    "            chosen_log_probs = chosen_log_probs[mask]\n",
    "            perplex.add_sentence(chosen_log_probs)\n",
    "        dev_perplexity = perplex.compute_perplexity()\n",
    "    print(f\"Epoch {epoch}: Loss {avg_train_loss:.4f}, Dev Perplexity = {dev_perplexity:.4f}\")\n",
    "end_training_time = time.time()\n",
    "total_time = end_training_time - start_training_time\n",
    "print(f\"Total training time for {20} epochs: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39ed24ac-400b-48d6-afb0-1db316a6371e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Test Perplexity: 304.7258641208873\n"
     ]
    }
   ],
   "source": [
    "model_lstm.eval()\n",
    "with torch.no_grad():\n",
    "    perplex = Perplexity()\n",
    "    for X_batch, Y_batch, lengths in test_loader:\n",
    "        logits, _ = model_lstm(X_batch)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        chosen_log_probs = log_probs.reshape(-1, log_probs.size(-1))[range(Y_batch.numel()), Y_batch.reshape(-1)]\n",
    "        mask = (Y_batch != word_dict.bos_idx).view(-1)  # Don't count bos_idx in the calculation\n",
    "        chosen_log_probs = chosen_log_probs[mask]\n",
    "        perplex.add_sentence(chosen_log_probs)\n",
    "    print(\"LSTM Test Perplexity:\", perplex.compute_perplexity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d55356-cdf6-4165-a68f-f6ae552fe700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
